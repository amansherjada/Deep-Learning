{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffebc2e5-d57a-410a-85c8-18e18cade7e9",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron (MLP)\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) is a type of artificial neural network that consists of multiple layers of interconnected neurons, including an input layer, one or more hidden layers, and an output layer. Here's a simplified intuition behind an MLP:\r\n",
    "![mlp](https://www.researchgate.net/publication/334609713/figure/fig1/AS:783455927406593@1563801857102/Multi-Layer-Perceptron-MLP-diagram-with-four-hidden-layers-and-a-collection-of-single.jpg)\n",
    "\n",
    "\r\n",
    "1. **Input Layer**:\r\n",
    "   - The input layer consists of neurons that represent the features of the input data. Each neuron corresponds to one feature, and the values of these neurons are fed as input to the network.\r\n",
    "\r\n",
    "2. **Hidden Layers**:\r\n",
    "   - The hidden layers are intermediate layers between the input and output layers. Each neuron in a hidden layer takes input from the neurons in the previous layer, performs a weighted sum of the inputs, adds a bias term, and applies an activation function to produce an output.\r\n",
    "   - The hidden layers allow the network to learn complex patterns and relationships in the data by transforming the input data through a series of nonlinear transformations.\r\n",
    "   - By adding more hidden layers and neurons, the network can capture increasingly complex features and representations of the input data.\r\n",
    "\r\n",
    "3. **Output Layer**:\r\n",
    "   - The output layer consists of neurons that produce the final output of the network. The number of neurons in the output layer depends on the type of task the network is performing. For example, in a binary classification task, there may be one neuron representing each class, while in a multi-class classification task, there may be multiple neurons, each corresponding to a different class.\r\n",
    "   - The output of each neuron in the output layer is typically passed through an activation function, which depends on the task. For example, in binary classification, a sigmoid or softmax activation function may be used to produce probabilities for each class.\r\n",
    "\r\n",
    "4. **Activation Functions**:\r\n",
    "   - Activation functions introduce nonlinearity to the network, allowing it to learn complex relationships in the data. Common activation functions used in MLPs include sigmoid, hyperbolic tangent (tanh), and rectified linear unit (ReLU).\r\n",
    "   - Sigmoid and tanh activation functions squash the output to a specific range (e.g., between 0 and 1 for sigmoid), while ReLU sets negative values to zero and leaves positive values unchanged.\r\n",
    "\r\n",
    "5. **Training**:\r\n",
    "   - During training, the network adjusts the weights and biases of the neurons using optimization algorithms such as gradient descent and backpropagation.\r\n",
    "   - Backpropagation calculates the gradient of the loss function with respect to the weights and biases of the network, allowing the weights to be updated in a direction that minimizes the loss.\r\n",
    "\r\n",
    "In summary, an MLP is a powerful and flexible neural network architecture capable of learning complex patterns and relationships in data. By stacking multiple layers of neurons and applying nonlinear activation functions, MLPs can model intricate mappings from input to output, making them suitable for a wide range of machine learning tasks, including classification, regression, and pattern recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4aeb2-8c37-4376-8520-c4bef56cda2f",
   "metadata": {},
   "source": [
    "1. **Perceptron with Sigmoid**:\n",
    "   - At its core, a perceptron is a single-layer neural network with binary output (0 or 1). When we add a sigmoid activation function to the output layer of a perceptron, it allows the network to produce continuous outputs between 0 and 1, which can be interpreted as probabilities.\n",
    "   - The sigmoid function smooths out the output, making it suitable for tasks like binary classification, where we want to estimate the probability of an input belonging to a particular class.\n",
    "\n",
    "![](https://i.stack.imgur.com/Waz75.png)\n",
    "\n",
    "2. **Adding Nodes in Hidden Layer**:\n",
    "   - In an MLP, we introduce one or more hidden layers between the input and output layers. Each node in a hidden layer acts as a perceptron, processing the input data and passing it through an activation function to produce an output.\n",
    "   - Adding more nodes to the hidden layer allows the network to learn more complex patterns and relationships in the data. Each node can capture different features or aspects of the input, leading to a richer representation of the data.\n",
    "\n",
    "\n",
    "3. **Adding Nodes in Input**:\n",
    "   - Adding more nodes to the input layer increases the dimensionality of the input space. Each node represents a different feature or attribute of the input data.\n",
    "   - By increasing the number of input nodes, the network can capture more detailed information about the input data, potentially improving its ability to learn and generalize from the data.\n",
    "\n",
    "4. **Adding Nodes in Output Layer**:\n",
    "   - Adding more nodes to the output layer is useful when dealing with multi-class classification tasks. Each node in the output layer corresponds to a different class, and the network learns to predict the probability of each class for a given input.\n",
    "   - By increasing the number of output nodes, the network can distinguish between more classes and make more fine-grained predictions.\n",
    "\n",
    "![](https://www.datasciencecentral.com/wp-content/uploads/2021/10/1140047167.png)\n",
    "\n",
    "5. **Deep Neural Network (DNN)**:\n",
    "   - A Deep Neural Network (DNN) refers to an MLP with multiple hidden layers. As we add more hidden layers to the network, it becomes deeper, allowing it to learn increasingly complex and abstract representations of the input data.\n",
    "   - Deep architectures enable the network to automatically extract hierarchical features from the data, with each layer learning to represent higher-level abstractions of the input.\n",
    "   - Deep learning models, such as DNNs, have shown remarkable performance in various tasks, including image recognition, natural language processing, and speech recognition.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*P2ukL6wXmFZAoURb0qlsfw.png)\n",
    "\n",
    "In summary, an MLP with sigmoid activation functions in its layers, along with various configurations of nodes in input, hidden, and output layers, forms a flexible and powerful framework for learning complex patterns in data, making it suitable for a wide range of machine learning tasks. Additionally, by adding more layers and nodes, we can build deep neural networks capable of capturing intricate relationships in the data and achieving state-of-the-art performance on challenging tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
